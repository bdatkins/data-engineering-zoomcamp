{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98b121ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46aed59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b9e4fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/01 20:40:40 WARN Utils: Your hostname, silverlining.local resolves to a loopback address: 127.0.0.1; using 192.168.0.224 instead (on interface en0)\n",
      "24/03/01 20:40:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/01 20:40:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f8d542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('taxi+_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1f87832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-01 20:40:54--  https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-01.csv\n",
      "Resolving nyc-tlc.s3.amazonaws.com (nyc-tlc.s3.amazonaws.com)... 52.217.120.49, 52.217.123.185, 52.216.77.108, ...\n",
      "Connecting to nyc-tlc.s3.amazonaws.com (nyc-tlc.s3.amazonaws.com)|52.217.120.49|:443... connected.\n",
      "HTTP request sent, awaiting response... 403 Forbidden\n",
      "2024-03-01 20:40:55 ERROR 403: Forbidden.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-01.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abc99011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-01 20:40:58--  https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-01.parquet\n",
      "Resolving d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)... 13.35.86.77, 13.35.86.31, 13.35.86.59, ...\n",
      "Connecting to d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)|13.35.86.77|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 308924937 (295M) [application/x-www-form-urlencoded]\n",
      "Saving to: ‘fhvhv_tripdata_2021-01.parquet.2’\n",
      "\n",
      "fhvhv_tripdata_2021 100%[===================>] 294.61M  40.8MB/s    in 9.2s    \n",
      "\n",
      "2024-03-01 20:41:08 (32.0 MB/s) - ‘fhvhv_tripdata_2021-01.parquet.2’ saved [308924937/308924937]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-01.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68025b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .parquet('fhvhv_tripdata_2021-01.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "077365d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
      "|hvfhs_license_num|dispatching_base_num|originating_base_num|   request_datetime|  on_scene_datetime|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|trip_miles|trip_time|base_passenger_fare|tolls| bcf|sales_tax|congestion_surcharge|airport_fee|tips|driver_pay|shared_request_flag|shared_match_flag|access_a_ride_flag|wav_request_flag|wav_match_flag|\n",
      "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
      "|           HV0003|              B02682|              B02682|2020-12-31 19:28:09|2020-12-31 19:31:42|2020-12-31 19:33:44|2020-12-31 19:49:07|         230|         166|      5.26|      923|              22.28|  0.0|0.67|     1.98|                2.75|       null| 0.0|     14.99|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02682|              B02682|2020-12-31 19:45:56|2020-12-31 19:55:19|2020-12-31 19:55:19|2020-12-31 20:18:21|         152|         167|      3.65|     1382|              18.36|  0.0|0.55|     1.63|                 0.0|       null| 0.0|     17.06|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02764|              B02764|2020-12-31 19:21:15|2020-12-31 19:22:41|2020-12-31 19:23:56|2020-12-31 19:38:05|         233|         142|      3.51|      849|              14.05|  0.0|0.48|     1.25|                2.75|       null|0.94|     12.98|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02764|              B02764|2020-12-31 19:39:12|2020-12-31 19:42:37|2020-12-31 19:42:51|2020-12-31 19:45:50|         142|         143|      0.74|      179|               7.91|  0.0|0.24|      0.7|                2.75|       null| 0.0|      7.41|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02764|              B02764|2020-12-31 19:46:11|2020-12-31 19:47:17|2020-12-31 19:48:14|2020-12-31 20:08:42|         143|          78|       9.2|     1228|              27.11|  0.0|0.81|     2.41|                2.75|       null| 0.0|     22.44|                  N|                N|                  |               N|             N|\n",
      "|           HV0005|              B02510|                null|2020-12-31 19:04:00|               null|2020-12-31 19:06:59|2020-12-31 19:43:01|          88|          42|     9.725|     2162|              28.11|  0.0|0.84|     2.49|                2.75|       null| 0.0|      28.9|                  N|                N|                 N|               N|             N|\n",
      "|           HV0005|              B02510|                null|2020-12-31 19:40:06|               null|2020-12-31 19:50:00|2020-12-31 20:04:57|          42|         151|     2.469|      897|              25.03|  0.0|0.75|     2.22|                 0.0|       null| 0.0|     15.01|                  N|                N|                 N|               N|             N|\n",
      "|           HV0003|              B02764|              B02764|2020-12-31 19:10:36|2020-12-31 19:12:28|2020-12-31 19:14:30|2020-12-31 19:50:27|          71|         226|     13.53|     2157|              29.67|  0.0|1.04|     3.08|                 0.0|       null| 0.0|      34.2|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02875|              B02875|2020-12-31 19:21:17|2020-12-31 19:22:25|2020-12-31 19:22:54|2020-12-31 19:30:20|         112|         255|       1.6|      446|               6.89|  0.0|0.21|     0.61|                 0.0|       null| 0.0|      6.26|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02875|              B02875|2020-12-31 19:36:57|2020-12-31 19:38:09|2020-12-31 19:40:12|2020-12-31 19:53:31|         255|         232|       3.2|      800|              11.51|  0.0|0.53|     1.03|                2.75|       null|2.82|     10.99|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02875|              B02875|2020-12-31 19:53:31|2020-12-31 19:56:21|2020-12-31 19:56:45|2020-12-31 20:17:42|         232|         198|      5.74|     1257|              17.18|  0.0|0.52|     1.52|                2.75|       null| 0.0|     17.61|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02835|              B02835|2020-12-31 19:22:58|2020-12-31 19:27:01|2020-12-31 19:29:04|2020-12-31 19:36:27|         113|          48|       1.8|      443|               8.18|  0.0|0.25|     0.73|                2.75|       null| 0.0|      6.12|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02835|              B02835|2020-12-31 19:46:44|2020-12-31 19:47:49|2020-12-31 19:48:56|2020-12-31 19:59:12|         239|          75|       2.9|      616|               13.1|  0.0|0.45|     1.17|                2.75|       null|0.94|      8.77|                  N|                N|                  |               N|             N|\n",
      "|           HV0004|              B02800|                null|2020-12-31 19:12:50|               null|2020-12-31 19:15:24|2020-12-31 19:38:31|         181|         237|      9.66|     1387|              32.95|  0.0| 0.0|     2.34|                2.75|       null| 0.0|      21.1|                  N|                N|                 N|               N|             N|\n",
      "|           HV0004|              B02800|                null|2020-12-31 19:35:32|               null|2020-12-31 19:45:00|2020-12-31 20:06:45|         236|          68|      4.38|     1305|              22.91|  0.0| 0.0|     1.63|                2.75|       null|3.43|     15.82|                  N|                N|                 N|               N|             N|\n",
      "|           HV0003|              B02682|              B02682|2020-12-31 19:10:22|2020-12-31 19:11:03|2020-12-31 19:11:53|2020-12-31 19:18:06|         256|         148|      2.03|      373|               7.84|  0.0|0.42|      0.7|                2.75|       null|2.82|      6.93|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02682|              B02682|2020-12-31 19:25:00|2020-12-31 19:26:31|2020-12-31 19:28:31|2020-12-31 19:41:40|          79|          80|      3.08|      789|               13.2|  0.0| 0.4|     1.17|                2.75|       null| 0.0|     11.54|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02682|              B02682|2020-12-31 19:44:56|2020-12-31 19:49:55|2020-12-31 19:50:49|2020-12-31 19:55:59|          17|         217|      1.17|      310|               7.91|  0.0|0.24|      0.7|                 0.0|       null| 0.0|      6.94|                  N|                N|                  |               N|             N|\n",
      "|           HV0005|              B02510|                null|2020-12-31 19:05:04|               null|2020-12-31 19:08:40|2020-12-31 19:39:39|          62|          29|    10.852|     1859|              31.18|  0.0|0.94|     2.77|                 0.0|       null| 0.0|     27.61|                  N|                N|                 N|               N|             N|\n",
      "|           HV0003|              B02836|              B02836|2020-12-31 19:40:44|2020-12-31 19:53:34|2020-12-31 19:53:48|2020-12-31 20:11:40|          22|          22|      3.52|     1072|              28.67|  0.0|0.86|     2.54|                 0.0|       null| 0.0|     17.64|                  N|                N|                  |               N|             N|\n",
      "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19ecceda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', originating_base_num='B02682', request_datetime=datetime.datetime(2020, 12, 31, 19, 28, 9), on_scene_datetime=datetime.datetime(2020, 12, 31, 19, 31, 42), pickup_datetime=datetime.datetime(2020, 12, 31, 19, 33, 44), dropoff_datetime=datetime.datetime(2020, 12, 31, 19, 49, 7), PULocationID=230, DOLocationID=166, trip_miles=5.26, trip_time=923, base_passenger_fare=22.28, tolls=0.0, bcf=0.67, sales_tax=1.98, congestion_surcharge=2.75, airport_fee=None, tips=0.0, driver_pay=14.99, shared_request_flag='N', shared_match_flag='N', access_a_ride_flag=' ', wav_request_flag='N', wav_match_flag='N')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cce22a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('originating_base_num', StringType(), True), StructField('request_datetime', TimestampType(), True), StructField('on_scene_datetime', TimestampType(), True), StructField('pickup_datetime', TimestampType(), True), StructField('dropoff_datetime', TimestampType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('trip_miles', DoubleType(), True), StructField('trip_time', LongType(), True), StructField('base_passenger_fare', DoubleType(), True), StructField('tolls', DoubleType(), True), StructField('bcf', DoubleType(), True), StructField('sales_tax', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True), StructField('airport_fee', DoubleType(), True), StructField('tips', DoubleType(), True), StructField('driver_pay', DoubleType(), True), StructField('shared_request_flag', StringType(), True), StructField('shared_match_flag', StringType(), True), StructField('access_a_ride_flag', StringType(), True), StructField('wav_request_flag', StringType(), True), StructField('wav_match_flag', StringType(), True)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb151410",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 101 fhvhv_tripdata_2021-01.parquet > head.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca46e70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAR1\u0015\u0004\u0015<\u0015HL\u0015\u0006\u0015\u0004\u0012\u0000\u0000\u001f�\b\u0000\u0000\u0000\u0000\u0000\u0000\r\n",
      "cc``�\b3000f��L�,\u0013\u0000�)Fs\u001e",
      "\u0000\u0000\u0000\u0015\u0000\u0015��S\u0015��/,\u0015���\u0002\u0015\u0004\u0015\u0006\u0015\u0006\u001c",
      "6\u0000(\u0006HV0005\u0018\u0006HV0003\u0000\u0000\u0000\u001f�\b\u0000\u0000\u0000\u0000\u0000\u0000\r\n",
      "L�oo\u001d",
      "W��\u000b",
      "�}\u0018[�\u0011A0\u000e\u0015AQ4M�JJI��*�G��5ܞ\u001d",
      "�\t#+'Qؑ�MXB��*X��q1�\\\\\\\f",
      "��\u0005=���\u0006\u0003��,�\u0019���AU�j�\u000f0�b����=+��NY\"ϟ�\u001d",
      "{�?�zֳR\b�����Vq�8t�M�/1��\u000f}h�%�G\r",
      "�ڏJ_�z\u0012��K�q���RI��9�˰ۇ0Ǯ\r",
      "�T��S\u0019^��\u001a�C���ð�g���\u000e)�\u0012R\u000e�q�\r",
      "g!\r",
      "�o{��=�0V>`�!��\u001e",
      "�\u0014C���`\r\n",
      "�\u001c",
      "R��N��.��>�,��ϼ;�\u0018�\u001c",
      "n�-�\u000f��$6�]X�y�~�o��O���\r\n",
      ";)�a��}֐�9�������)��e݄4?\t\u000f�\\�h+b/��6��\u0016��\u000f��B\r\n",
      "�}Ա]]j��A��|\u0014v�\u001c",
      "m��l_7�c�:�Z��(�Ű�5����O��8c�R�Å�3d{�a؍%|�{��ޮ��_�\u0005�Ga�.�)�`�bK\u001c",
      "\u000el����k���v;k��S_�\u001bk�\u0002\f",
      "��d�\u001b�<�\u001a���o�ϳ��{O�\u001e",
      "�}�~?${ǜ{��9\u001c",
      "��.����4v\u0005�u\r\n",
      "y\u0015\u000e�^ϳk��#�\"�-{�ֿ\r",
      "'aO�ѕv\u001e",
      "��r\u0014�O���\t�C)�M���Ν�^�qͺ��\u0014�\u001c",
      "����l�r\u000e%��S��K���\u000b",
      "{���~4�z�OB�l\u001f��X){ԡ�پj��ą>b1�CN��N��t\u0016v\"[.塶v���C��=��;��tv��\u001b�kі-��m_[E\u001e",
      "�]G-�i乷���K��4���3ʓ=\u0002{��x������\u001c",
      "'{O�K}��Ȗ�٫G�T\u0019�y(��vh�v�m��w������/\u001d",
      "k����Ӱ\u001e",
      "BgkW�\u001av���/xM\u000eS=�\u001bjO�С9�\r",
      "W\u0012{����N�\u0001�&�\u000f���O��S������Ξ=α�Zڮ<\r",
      ";S��Sن|l��V.�\u0013_���h�\u0016����\u0012�&��\u001d",
      "}��mȇv��0q�Cmg{^\u0013V\"�c�\u001d",
      "Kc��́�m\u000e@���J?���o��6~�}\u0011�<�k;���\b�I��d�f�E�v�\u001b�\u001a\"�P\u0002g�6��\t��\u0019ƣ��\u0000\u001f�c�I�?�\u00136�5ڦ�S����p߮eNM\u0019;�\u000f�t۝p\r\n",
      "J��e�.%أ߭qh���d�~]9�vq</{�i��r~�l֐���#��l�1g\u0003;\u0015b��MlG�\u0014��l!,�ݫY�>��\u001d",
      "n{e�\u001d",
      "�#a���\u0012��>妪=\r",
      "�\u0005���8�q����\u0019��v�Z;73\u0006�ήL%�sn�>��D�B��?n��ž���\u0006�d��ڍO�����ۧ��l���6�p\u001c",
      "���Y>�/졘\u0011e+�\u001c",
      "\u001b6Sz�/���\u0013K\u001asH[��<K�aذ1mߴa\u001e",
      "{�\u0016�z��s;7�ڐ��\u0001����M�72����R�l��Z�'�\b/ra_k�܋v�m��B�����#����[\u0017���cv�\u0016;µ�IJ�}Dowa늙b1�ޜ��(rA\u001c",
      "a\u0016�0|�嵧�����v�f�y4G���S؊7�\u001f\u000b",
      "��p`<�N��=��يv\t�\u001c",
      "m;�v�wF;\u0011\u000f؅cjZs�M�l�\u000f,�h7��0�\u0013�V\\c;q�G�'�sa�0g�\"����ގO\u0018�\\�\r",
      "k�ؚg}5O�>����q4�%\u001b��9�.<�\\�}��q\u0012S\u001b�ٷ4�R��t=\u001f�\u000e�vq(v\u001aڮ���~�00fg&.��+��^4\u0007co��r�C0���Ͱ�v&w�g����\u0016�<Q��T��f��G\u0017�\u001e",
      "�H�ln�޸�\u000f}0�qʳ]d\u001e",
      "&�\u0010���6�<S�\b؃�ھ�����6c1Oc�h�ui͚����;0W\u0015��-l\t\u0012�:�Cmp\\�9<\u000eGie����?Ѷ\u0010�7�\u0016���F����\u001f)p���م9|����-\u000f��\u000f�\u0013[��C{.r\u0011c����9�ض�T{�\u0017b��'YxaF�<�}{\"-�k�o\b�1�:�!]*��\bT��i\u0002��,\u0016_���>����v���\u0019\u0001�\f",
      "���6�`g��\u0015%\u000e�=��\u0016�����]ɝ,�o�7����z�;�Nx��{�ɋ�.��\u001d",
      "����\u0017\f",
      "������ߏ��8�S\r\n",
      "=��nn_f�v���j�1�cy��\u0005/��f�=�Б�\u0014�l؆�슿��ؕ��\u000f<�ж�~��̛\u00151�u�pf�3쉛��/Q46W��T�����Aw�\u0005\u0003�l�&��\bɪ��\u0011_�8�\u0016���5\u0018u;=���{n+Ȧ1�|�\u001b�^j7W��g��\u000en-x�\r",
      "����jO��c�\u0012��9ul�Mkq^��t�P&��H�8�Z\u001c",
      "��m�}��&\u001e",
      "7k��%̶��XY5��\\�$w��c\u000f\u001b�|lw>�\u0016p�3�n���\u0014\b�����f8�Y�1�v3/�c�C�Ǫe����МȪ�\u001d",
      "c�,��9�̇����=�����6W�kG��#����E�;fB,A�ڣ��\u0013��\u001d",
      "a[��c��ƪMq�\r",
      "w᎟��1�\u000eɂA[�\u000f�\u0015�xM���r��/\u0018-\u0012\t\u000eD�<��cP;g=�L��+�`��D mW@�\u0013�_�6��DEԶ�Aq��\u0010M���2\u0007W\u001e",
      "1���\f",
      "�C�#.�̛�7��D#f�}�d��}�a|*W��\u0002a�w\b�l��}5�O$��e4�g;�_\u0012l��\"7؏Ă��wfB�D �]�+(�\u0000��c��\u0017��ɖ��vLڸc��~�f������l6\u0016�L�n�v�,�����M��س<�AY�\u0014����B0�Jq��\u0011���]R��0^\u001c",
      "-'Ü�Żk��\u0016[�^��\u0013`+@8����|����sϼ�({�\u0010_���ϭ��ݐ\u0005-�s|d��\u00000Of#��%w\u001f\u0007l:�\u001e",
      "�Z�̬C`�j\u001f}��o������-�eێ�e����ðz�\u0016�\u0004��N�-�zH�8\u0006��-ԉ\u0007LJ:�.�#�Օ\u0014\u000b",
      "#{���it\u0004#<��e\u001c",
      "՞����\u0002�\u00075e3�2��`���;7X�9G�Z1��6�\u0012\u0007\u000b",
      "M�C�Ck��=����\u0019������Ɣ\u0014��\u0007���lF�˛�[fb��x��h���o'��ݘ�o+�_�%x�L�t�\u0017��ҊMw6$&�e\u0005z9�O��>�\u001c",
      "y/#Y�\\k�\u0010\r\n",
      "��7O�^��\u001f�����\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 head.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68f7aed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
      "|hvfhs_license_num|dispatching_base_num|originating_base_num|   request_datetime|  on_scene_datetime|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|trip_miles|trip_time|base_passenger_fare|tolls| bcf|sales_tax|congestion_surcharge|airport_fee|tips|driver_pay|shared_request_flag|shared_match_flag|access_a_ride_flag|wav_request_flag|wav_match_flag|\n",
      "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
      "|           HV0003|              B02682|              B02682|2020-12-31 19:28:09|2020-12-31 19:31:42|2020-12-31 19:33:44|2020-12-31 19:49:07|         230|         166|      5.26|      923|              22.28|  0.0|0.67|     1.98|                2.75|       null| 0.0|     14.99|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02682|              B02682|2020-12-31 19:45:56|2020-12-31 19:55:19|2020-12-31 19:55:19|2020-12-31 20:18:21|         152|         167|      3.65|     1382|              18.36|  0.0|0.55|     1.63|                 0.0|       null| 0.0|     17.06|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02764|              B02764|2020-12-31 19:21:15|2020-12-31 19:22:41|2020-12-31 19:23:56|2020-12-31 19:38:05|         233|         142|      3.51|      849|              14.05|  0.0|0.48|     1.25|                2.75|       null|0.94|     12.98|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02764|              B02764|2020-12-31 19:39:12|2020-12-31 19:42:37|2020-12-31 19:42:51|2020-12-31 19:45:50|         142|         143|      0.74|      179|               7.91|  0.0|0.24|      0.7|                2.75|       null| 0.0|      7.41|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02764|              B02764|2020-12-31 19:46:11|2020-12-31 19:47:17|2020-12-31 19:48:14|2020-12-31 20:08:42|         143|          78|       9.2|     1228|              27.11|  0.0|0.81|     2.41|                2.75|       null| 0.0|     22.44|                  N|                N|                  |               N|             N|\n",
      "|           HV0005|              B02510|                null|2020-12-31 19:04:00|               null|2020-12-31 19:06:59|2020-12-31 19:43:01|          88|          42|     9.725|     2162|              28.11|  0.0|0.84|     2.49|                2.75|       null| 0.0|      28.9|                  N|                N|                 N|               N|             N|\n",
      "|           HV0005|              B02510|                null|2020-12-31 19:40:06|               null|2020-12-31 19:50:00|2020-12-31 20:04:57|          42|         151|     2.469|      897|              25.03|  0.0|0.75|     2.22|                 0.0|       null| 0.0|     15.01|                  N|                N|                 N|               N|             N|\n",
      "|           HV0003|              B02764|              B02764|2020-12-31 19:10:36|2020-12-31 19:12:28|2020-12-31 19:14:30|2020-12-31 19:50:27|          71|         226|     13.53|     2157|              29.67|  0.0|1.04|     3.08|                 0.0|       null| 0.0|      34.2|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02875|              B02875|2020-12-31 19:21:17|2020-12-31 19:22:25|2020-12-31 19:22:54|2020-12-31 19:30:20|         112|         255|       1.6|      446|               6.89|  0.0|0.21|     0.61|                 0.0|       null| 0.0|      6.26|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02875|              B02875|2020-12-31 19:36:57|2020-12-31 19:38:09|2020-12-31 19:40:12|2020-12-31 19:53:31|         255|         232|       3.2|      800|              11.51|  0.0|0.53|     1.03|                2.75|       null|2.82|     10.99|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02875|              B02875|2020-12-31 19:53:31|2020-12-31 19:56:21|2020-12-31 19:56:45|2020-12-31 20:17:42|         232|         198|      5.74|     1257|              17.18|  0.0|0.52|     1.52|                2.75|       null| 0.0|     17.61|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02835|              B02835|2020-12-31 19:22:58|2020-12-31 19:27:01|2020-12-31 19:29:04|2020-12-31 19:36:27|         113|          48|       1.8|      443|               8.18|  0.0|0.25|     0.73|                2.75|       null| 0.0|      6.12|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02835|              B02835|2020-12-31 19:46:44|2020-12-31 19:47:49|2020-12-31 19:48:56|2020-12-31 19:59:12|         239|          75|       2.9|      616|               13.1|  0.0|0.45|     1.17|                2.75|       null|0.94|      8.77|                  N|                N|                  |               N|             N|\n",
      "|           HV0004|              B02800|                null|2020-12-31 19:12:50|               null|2020-12-31 19:15:24|2020-12-31 19:38:31|         181|         237|      9.66|     1387|              32.95|  0.0| 0.0|     2.34|                2.75|       null| 0.0|      21.1|                  N|                N|                 N|               N|             N|\n",
      "|           HV0004|              B02800|                null|2020-12-31 19:35:32|               null|2020-12-31 19:45:00|2020-12-31 20:06:45|         236|          68|      4.38|     1305|              22.91|  0.0| 0.0|     1.63|                2.75|       null|3.43|     15.82|                  N|                N|                 N|               N|             N|\n",
      "|           HV0003|              B02682|              B02682|2020-12-31 19:10:22|2020-12-31 19:11:03|2020-12-31 19:11:53|2020-12-31 19:18:06|         256|         148|      2.03|      373|               7.84|  0.0|0.42|      0.7|                2.75|       null|2.82|      6.93|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02682|              B02682|2020-12-31 19:25:00|2020-12-31 19:26:31|2020-12-31 19:28:31|2020-12-31 19:41:40|          79|          80|      3.08|      789|               13.2|  0.0| 0.4|     1.17|                2.75|       null| 0.0|     11.54|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02682|              B02682|2020-12-31 19:44:56|2020-12-31 19:49:55|2020-12-31 19:50:49|2020-12-31 19:55:59|          17|         217|      1.17|      310|               7.91|  0.0|0.24|      0.7|                 0.0|       null| 0.0|      6.94|                  N|                N|                  |               N|             N|\n",
      "|           HV0005|              B02510|                null|2020-12-31 19:05:04|               null|2020-12-31 19:08:40|2020-12-31 19:39:39|          62|          29|    10.852|     1859|              31.18|  0.0|0.94|     2.77|                 0.0|       null| 0.0|     27.61|                  N|                N|                 N|               N|             N|\n",
      "|           HV0003|              B02836|              B02836|2020-12-31 19:40:44|2020-12-31 19:53:34|2020-12-31 19:53:48|2020-12-31 20:11:40|          22|          22|      3.52|     1072|              28.67|  0.0|0.86|     2.54|                 0.0|       null| 0.0|     17.64|                  N|                N|                  |               N|             N|\n",
      "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb101c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6fdcbad",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "Could not open Parquet input source '<Buffer>': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_pandas \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhead.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parquet.py:509\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    507\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mread(\n\u001b[1;32m    510\u001b[0m     path,\n\u001b[1;32m    511\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m    512\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    513\u001b[0m     use_nullable_dtypes\u001b[38;5;241m=\u001b[39muse_nullable_dtypes,\n\u001b[1;32m    514\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    516\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parquet.py:227\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, use_nullable_dtypes, dtype_backend, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m path_or_handle, handles, kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilesystem\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_path_or_handle(\n\u001b[1;32m    221\u001b[0m     path,\n\u001b[1;32m    222\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilesystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    223\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    224\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m )\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 227\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    228\u001b[0m         path_or_handle, columns\u001b[38;5;241m=\u001b[39mcolumns, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    229\u001b[0m     )\n\u001b[1;32m    230\u001b[0m     result \u001b[38;5;241m=\u001b[39m pa_table\u001b[38;5;241m.\u001b[39mto_pandas(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mto_pandas_kwargs)\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyarrow/parquet/core.py:2926\u001b[0m, in \u001b[0;36mread_table\u001b[0;34m(source, columns, use_threads, metadata, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit)\u001b[0m\n\u001b[1;32m   2919\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2920\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m keyword is no longer supported with the new \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2921\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets-based implementation. Specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2922\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_legacy_dataset=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to temporarily recover the old \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2923\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbehaviour.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2924\u001b[0m     )\n\u001b[1;32m   2925\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2926\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m _ParquetDatasetV2(\n\u001b[1;32m   2927\u001b[0m         source,\n\u001b[1;32m   2928\u001b[0m         schema\u001b[38;5;241m=\u001b[39mschema,\n\u001b[1;32m   2929\u001b[0m         filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[1;32m   2930\u001b[0m         partitioning\u001b[38;5;241m=\u001b[39mpartitioning,\n\u001b[1;32m   2931\u001b[0m         memory_map\u001b[38;5;241m=\u001b[39mmemory_map,\n\u001b[1;32m   2932\u001b[0m         read_dictionary\u001b[38;5;241m=\u001b[39mread_dictionary,\n\u001b[1;32m   2933\u001b[0m         buffer_size\u001b[38;5;241m=\u001b[39mbuffer_size,\n\u001b[1;32m   2934\u001b[0m         filters\u001b[38;5;241m=\u001b[39mfilters,\n\u001b[1;32m   2935\u001b[0m         ignore_prefixes\u001b[38;5;241m=\u001b[39mignore_prefixes,\n\u001b[1;32m   2936\u001b[0m         pre_buffer\u001b[38;5;241m=\u001b[39mpre_buffer,\n\u001b[1;32m   2937\u001b[0m         coerce_int96_timestamp_unit\u001b[38;5;241m=\u001b[39mcoerce_int96_timestamp_unit,\n\u001b[1;32m   2938\u001b[0m         thrift_string_size_limit\u001b[38;5;241m=\u001b[39mthrift_string_size_limit,\n\u001b[1;32m   2939\u001b[0m         thrift_container_size_limit\u001b[38;5;241m=\u001b[39mthrift_container_size_limit,\n\u001b[1;32m   2940\u001b[0m     )\n\u001b[1;32m   2941\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m   2942\u001b[0m     \u001b[38;5;66;03m# fall back on ParquetFile for simple cases when pyarrow.dataset\u001b[39;00m\n\u001b[1;32m   2943\u001b[0m     \u001b[38;5;66;03m# module is not available\u001b[39;00m\n\u001b[1;32m   2944\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyarrow/parquet/core.py:2466\u001b[0m, in \u001b[0;36m_ParquetDatasetV2.__init__\u001b[0;34m(self, path_or_paths, filesystem, filters, partitioning, read_dictionary, buffer_size, memory_map, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, schema, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, **kwargs)\u001b[0m\n\u001b[1;32m   2462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m single_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2463\u001b[0m     fragment \u001b[38;5;241m=\u001b[39m parquet_format\u001b[38;5;241m.\u001b[39mmake_fragment(single_file, filesystem)\n\u001b[1;32m   2465\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mFileSystemDataset(\n\u001b[0;32m-> 2466\u001b[0m         [fragment], schema\u001b[38;5;241m=\u001b[39mschema \u001b[38;5;129;01mor\u001b[39;00m fragment\u001b[38;5;241m.\u001b[39mphysical_schema,\n\u001b[1;32m   2467\u001b[0m         \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mparquet_format,\n\u001b[1;32m   2468\u001b[0m         filesystem\u001b[38;5;241m=\u001b[39mfragment\u001b[38;5;241m.\u001b[39mfilesystem\n\u001b[1;32m   2469\u001b[0m     )\n\u001b[1;32m   2470\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2472\u001b[0m \u001b[38;5;66;03m# check partitioning to enable dictionary encoding\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyarrow/_dataset.pyx:1004\u001b[0m, in \u001b[0;36mpyarrow._dataset.Fragment.physical_schema.__get__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyarrow/error.pxi:144\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyarrow/error.pxi:100\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: Could not open Parquet input source '<Buffer>': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file."
     ]
    }
   ],
   "source": [
    "df_pandas = pd.read_parquet('head.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8feec3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = pd.read_parquet('fhvhv_tripdata_2021-01.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d8c1fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hvfhs_license_num               object\n",
       "dispatching_base_num            object\n",
       "originating_base_num            object\n",
       "request_datetime        datetime64[ns]\n",
       "on_scene_datetime       datetime64[ns]\n",
       "pickup_datetime         datetime64[ns]\n",
       "dropoff_datetime        datetime64[ns]\n",
       "PULocationID                     int64\n",
       "DOLocationID                     int64\n",
       "trip_miles                     float64\n",
       "trip_time                        int64\n",
       "base_passenger_fare            float64\n",
       "tolls                          float64\n",
       "bcf                            float64\n",
       "sales_tax                      float64\n",
       "congestion_surcharge           float64\n",
       "airport_fee                    float64\n",
       "tips                           float64\n",
       "driver_pay                     float64\n",
       "shared_request_flag             object\n",
       "shared_match_flag               object\n",
       "access_a_ride_flag              object\n",
       "wav_request_flag                object\n",
       "wav_match_flag                  object\n",
       "dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf5598e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'iteritems'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mh/qk0w87w95zv9zfrzst01dzr40000gn/T/ipykernel_86574/2729947182.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_pandas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/dev/projects/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m             \u001b[0mhas_pandas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_pandas\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0;31m# Create a DataFrame from pandas DataFrame.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m             return super(SparkSession, self).createDataFrame(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m    892\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             )\n\u001b[1;32m    894\u001b[0m         return self._create_dataframe(\n",
      "\u001b[0;32m~/dev/projects/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    432\u001b[0m                         \u001b[0;34m\"has been set to false.\\n  %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m                     )\n\u001b[1;32m    434\u001b[0m                     \u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m         \u001b[0mconverted_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_from_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimezone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/projects/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, pdf, schema, timezone)\u001b[0m\n\u001b[1;32m    470\u001b[0m                                 \u001b[0mcopied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m                             \u001b[0mpdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mshould_localize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_timestamp_ntz_preferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseries\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m                     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mshould_localize\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtz\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_series_convert_timestamps_tz_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimezone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5985\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5986\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5987\u001b[0m         ):\n\u001b[1;32m   5988\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'iteritems'"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(df_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2755f3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas.iteritems = df_pandas.items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "608741c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.readRDDFromFile.\n: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.api.java.JavaRDD$.readRDDFromInputStream(JavaRDD.scala:252)\n\tat org.apache.spark.api.java.JavaRDD$.readRDDFromFile(JavaRDD.scala:239)\n\tat org.apache.spark.api.python.PythonRDD$.readRDDFromFile(PythonRDD.scala:274)\n\tat org.apache.spark.api.python.PythonRDD.readRDDFromFile(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(df_pandas)\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/dev/projects/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/session.py:891\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    888\u001b[0m     has_pandas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pandas\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m--> 891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m    892\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m    893\u001b[0m     )\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(\n\u001b[1;32m    895\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    896\u001b[0m )\n",
      "File \u001b[0;32m~/dev/projects/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/conversion.py:437\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    436\u001b[0m converted_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_from_pandas(data, schema, timezone)\n\u001b[0;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(converted_data, schema, samplingRatio, verifySchema)\n",
      "File \u001b[0;32m~/dev/projects/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/session.py:936\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    934\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromLocal(\u001b[38;5;28mmap\u001b[39m(prepare, data), schema)\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    938\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[0;32m~/dev/projects/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/session.py:648\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;66;03m# convert python objects to sql data\u001b[39;00m\n\u001b[1;32m    647\u001b[0m internal_data \u001b[38;5;241m=\u001b[39m [struct\u001b[38;5;241m.\u001b[39mtoInternal(row) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m tupled_data]\n\u001b[0;32m--> 648\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39mparallelize(internal_data), struct\n",
      "File \u001b[0;32m~/dev/projects/spark/spark-3.3.2-bin-hadoop3/python/pyspark/context.py:674\u001b[0m, in \u001b[0;36mSparkContext.parallelize\u001b[0;34m(self, c, numSlices)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonParallelizeServer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc(), numSlices)\n\u001b[0;32m--> 674\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialize_to_jvm(c, serializer, reader_func, createRDDServer)\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m RDD(jrdd, \u001b[38;5;28mself\u001b[39m, serializer)\n",
      "File \u001b[0;32m~/dev/projects/spark/spark-3.3.2-bin-hadoop3/python/pyspark/context.py:720\u001b[0m, in \u001b[0;36mSparkContext._serialize_to_jvm\u001b[0;34m(self, data, serializer, reader_func, createRDDServer)\u001b[0m\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m         tempFile\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 720\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reader_func(tempFile\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;66;03m# we eagerly reads the file so we can delete right after.\u001b[39;00m\n\u001b[1;32m    723\u001b[0m     os\u001b[38;5;241m.\u001b[39munlink(tempFile\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[0;32m~/dev/projects/spark/spark-3.3.2-bin-hadoop3/python/pyspark/context.py:668\u001b[0m, in \u001b[0;36mSparkContext.parallelize.<locals>.reader_func\u001b[0;34m(temp_filename)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreader_func\u001b[39m(temp_filename: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JavaObject:\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 668\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD\u001b[38;5;241m.\u001b[39mreadRDDFromFile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc, temp_filename, numSlices)\n",
      "File \u001b[0;32m~/dev/projects/spark/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/dev/projects/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/dev/projects/spark/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.readRDDFromFile.\n: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.api.java.JavaRDD$.readRDDFromInputStream(JavaRDD.scala:252)\n\tat org.apache.spark.api.java.JavaRDD$.readRDDFromFile(JavaRDD.scala:239)\n\tat org.apache.spark.api.python.PythonRDD$.readRDDFromFile(PythonRDD.scala:274)\n\tat org.apache.spark.api.python.PythonRDD.readRDDFromFile(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(df_pandas).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "053174d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.readRDDFromFile.\n: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.api.java.JavaRDD$.readRDDFromInputStream(JavaRDD.scala:252)\n\tat org.apache.spark.api.java.JavaRDD$.readRDDFromFile(JavaRDD.scala:239)\n\tat org.apache.spark.api.python.PythonRDD$.readRDDFromFile(PythonRDD.scala:274)\n\tat org.apache.spark.api.python.PythonRDD.readRDDFromFile(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(df_pandas)\u001b[38;5;241m.\u001b[39mschema\n",
      "File \u001b[0;32m~/dev/projects/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/session.py:891\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    888\u001b[0m     has_pandas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pandas\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m--> 891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m    892\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m    893\u001b[0m     )\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(\n\u001b[1;32m    895\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    896\u001b[0m )\n",
      "File \u001b[0;32m~/dev/projects/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/conversion.py:437\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    436\u001b[0m converted_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_from_pandas(data, schema, timezone)\n\u001b[0;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(converted_data, schema, samplingRatio, verifySchema)\n",
      "File \u001b[0;32m~/dev/projects/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/session.py:936\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    934\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromLocal(\u001b[38;5;28mmap\u001b[39m(prepare, data), schema)\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    938\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[0;32m~/dev/projects/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/session.py:648\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;66;03m# convert python objects to sql data\u001b[39;00m\n\u001b[1;32m    647\u001b[0m internal_data \u001b[38;5;241m=\u001b[39m [struct\u001b[38;5;241m.\u001b[39mtoInternal(row) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m tupled_data]\n\u001b[0;32m--> 648\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39mparallelize(internal_data), struct\n",
      "File \u001b[0;32m~/dev/projects/spark/spark-3.3.2-bin-hadoop3/python/pyspark/context.py:674\u001b[0m, in \u001b[0;36mSparkContext.parallelize\u001b[0;34m(self, c, numSlices)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonParallelizeServer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc(), numSlices)\n\u001b[0;32m--> 674\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialize_to_jvm(c, serializer, reader_func, createRDDServer)\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m RDD(jrdd, \u001b[38;5;28mself\u001b[39m, serializer)\n",
      "File \u001b[0;32m~/dev/projects/spark/spark-3.3.2-bin-hadoop3/python/pyspark/context.py:720\u001b[0m, in \u001b[0;36mSparkContext._serialize_to_jvm\u001b[0;34m(self, data, serializer, reader_func, createRDDServer)\u001b[0m\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m         tempFile\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 720\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reader_func(tempFile\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;66;03m# we eagerly reads the file so we can delete right after.\u001b[39;00m\n\u001b[1;32m    723\u001b[0m     os\u001b[38;5;241m.\u001b[39munlink(tempFile\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[0;32m~/dev/projects/spark/spark-3.3.2-bin-hadoop3/python/pyspark/context.py:668\u001b[0m, in \u001b[0;36mSparkContext.parallelize.<locals>.reader_func\u001b[0;34m(temp_filename)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreader_func\u001b[39m(temp_filename: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JavaObject:\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 668\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD\u001b[38;5;241m.\u001b[39mreadRDDFromFile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc, temp_filename, numSlices)\n",
      "File \u001b[0;32m~/dev/projects/spark/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/dev/projects/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/dev/projects/spark/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.readRDDFromFile.\n: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.api.java.JavaRDD$.readRDDFromInputStream(JavaRDD.scala:252)\n\tat org.apache.spark.api.java.JavaRDD$.readRDDFromFile(JavaRDD.scala:239)\n\tat org.apache.spark.api.python.PythonRDD$.readRDDFromFile(PythonRDD.scala:274)\n\tat org.apache.spark.api.python.PythonRDD.readRDDFromFile(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(df_pandas).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15935fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b137758b",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('hvfhs_license_num', types.StringType(), True),\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('originating_base_num', types.StringType(), True),\n",
    "    types.StructField('request_datetime', types.TimestampType(), True),\n",
    "    types.StructField('on_scene_datetime', types.TimestampType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.LongType(), True),\n",
    "    types.StructField('DOLocationID', types.LongType(), True),\n",
    "    \n",
    "    types.StructField('shared_request_flag', types.StringType(), True)\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a32e05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .parquet('fhvhv_tripdata_2021-01.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c8cc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8fe2cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', originating_base_num='B02682', request_datetime=datetime.datetime(2020, 12, 31, 19, 28, 9), on_scene_datetime=datetime.datetime(2020, 12, 31, 19, 31, 42), pickup_datetime=datetime.datetime(2020, 12, 31, 19, 33, 44), dropoff_datetime=datetime.datetime(2020, 12, 31, 19, 49, 7), PULocationID=230, DOLocationID=166, shared_request_flag='N')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a8dece7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[hvfhs_license_num: string, dispatching_base_num: string, originating_base_num: string, request_datetime: timestamp, on_scene_datetime: timestamp, pickup_datetime: timestamp, dropoff_datetime: timestamp, PULocationID: bigint, DOLocationID: bigint, shared_request_flag: string]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad38459a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c1a169f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/Users/barbaraatkins/dev/projects/spark/notebooks/fhvhv/2021/01 already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfhvhv/2021/01/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/dev/projects/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/readwriter.py:1140\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mparquet(path)\n",
      "File \u001b[0;32m~/dev/projects/spark/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/dev/projects/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: path file:/Users/barbaraatkins/dev/projects/spark/notebooks/fhvhv/2021/01 already exists."
     ]
    }
   ],
   "source": [
    "df.write.parquet('fhvhv/2021/01/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ac081251",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('fhvhv/2021/01/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0126948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+-------------------+\n",
      "|hvfhs_license_num|dispatching_base_num|originating_base_num|   request_datetime|  on_scene_datetime|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|shared_request_flag|\n",
      "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+-------------------+\n",
      "|           HV0003|              B02872|              B02872|2021-01-16 18:43:34|2021-01-16 18:50:39|2021-01-16 18:52:08|2021-01-16 19:28:08|         138|         121|                  N|\n",
      "|           HV0003|              B02875|              B02875|2021-01-24 16:42:55|2021-01-24 16:44:42|2021-01-24 16:44:54|2021-01-24 17:10:13|         140|         265|                  N|\n",
      "|           HV0003|              B02765|              B02765|2021-01-15 14:51:13|2021-01-15 14:55:49|2021-01-15 14:56:27|2021-01-15 15:15:58|          38|         130|                  N|\n",
      "|           HV0003|              B02872|              B02872|2021-01-03 08:53:25|2021-01-03 08:56:21|2021-01-03 08:56:21|2021-01-03 09:10:04|         246|         141|                  N|\n",
      "|           HV0003|              B02617|              B02617|2021-01-04 02:41:55|2021-01-04 02:48:47|2021-01-04 02:48:56|2021-01-04 02:53:54|         147|         159|                  N|\n",
      "|           HV0003|              B02880|              B02880|2021-01-08 11:28:19|2021-01-08 11:30:02|2021-01-08 11:30:53|2021-01-08 11:39:37|          43|         141|                  N|\n",
      "|           HV0003|              B02836|              B02836|2021-01-19 02:45:17|2021-01-19 02:55:18|2021-01-19 02:55:18|2021-01-19 03:18:14|          36|          39|                  N|\n",
      "|           HV0003|              B02764|              B02764|2021-01-04 02:35:00|2021-01-04 02:36:05|2021-01-04 02:37:40|2021-01-04 02:51:35|          91|         188|                  N|\n",
      "|           HV0003|              B02884|              B02884|2021-01-07 10:41:14|2021-01-07 10:42:31|2021-01-07 10:44:31|2021-01-07 10:53:00|         186|         164|                  N|\n",
      "|           HV0003|              B02875|              B02875|2021-01-08 20:45:46|2021-01-08 20:47:34|2021-01-08 20:49:15|2021-01-08 20:52:02|         242|         248|                  N|\n",
      "|           HV0003|              B02876|              B02876|2021-01-23 03:13:49|2021-01-23 03:20:44|2021-01-23 03:20:53|2021-01-23 03:29:17|         157|         160|                  N|\n",
      "|           HV0003|              B02887|              B02887|2021-01-13 02:37:28|2021-01-13 02:42:29|2021-01-13 02:44:30|2021-01-13 02:49:58|         205|         205|                  N|\n",
      "|           HV0005|              B02510|                null|2021-01-19 13:40:20|               null|2021-01-19 13:42:26|2021-01-19 14:06:15|          52|          50|                  N|\n",
      "|           HV0005|              B02510|                null|2021-01-19 05:47:42|               null|2021-01-19 05:55:07|2021-01-19 06:21:05|         138|          87|                  N|\n",
      "|           HV0003|              B02617|              B02617|2021-01-02 10:35:31|2021-01-02 10:37:17|2021-01-02 10:38:36|2021-01-02 11:02:19|          89|          65|                  N|\n",
      "|           HV0003|              B02765|              B02765|2021-01-11 02:32:06|2021-01-11 02:37:12|2021-01-11 02:37:18|2021-01-11 02:57:27|         255|         170|                  N|\n",
      "|           HV0003|              B02864|              B02864|2021-01-04 15:38:24|2021-01-04 15:39:12|2021-01-04 15:40:28|2021-01-04 15:45:30|         174|         174|                  N|\n",
      "|           HV0003|              B02887|              B02887|2021-01-29 14:44:54|2021-01-29 14:46:52|2021-01-29 14:46:52|2021-01-29 15:05:14|          13|         237|                  N|\n",
      "|           HV0003|              B02887|              B02887|2021-01-14 04:42:55|2021-01-14 04:44:53|2021-01-14 04:46:53|2021-01-14 04:50:53|         130|         130|                  N|\n",
      "|           HV0003|              B02835|              B02835|2021-01-27 18:57:52|2021-01-27 18:59:16|2021-01-27 19:01:17|2021-01-27 19:21:30|         228|          61|                  N|\n",
      "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "24/03/01 21:33:41 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /private/var/folders/mh/qk0w87w95zv9zfrzst01dzr40000gn/T/blockmgr-c370c0ec-f888-4b18-9d18-750f554bfbee. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /private/var/folders/mh/qk0w87w95zv9zfrzst01dzr40000gn/T/blockmgr-c370c0ec-f888-4b18-9d18-750f554bfbee\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:171)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:110)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1206)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:374)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:370)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:370)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:365)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2040)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:95)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$23(SparkContext.scala:2150)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1484)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2150)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$35(SparkContext.scala:670)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2091f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
