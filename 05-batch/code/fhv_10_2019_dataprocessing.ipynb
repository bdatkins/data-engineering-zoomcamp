{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e461a990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ee6dc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27f02512",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/02 20:36:29 WARN Utils: Your hostname, silverlining.local resolves to a loopback address: 127.0.0.1; using 192.168.0.224 instead (on interface en0)\n",
      "24/03/02 20:36:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/02 20:36:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c637105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.1'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd8b48e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('fhv_tripdata_2019-10.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f7ca4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46702301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B00009|2019-10-01 00:23:00|2019-10-01 00:35:00|         264|         264|   NULL|                B00009|\n",
      "|              B00013|2019-10-01 00:11:29|2019-10-01 00:13:22|         264|         264|   NULL|                B00013|\n",
      "|              B00014|2019-10-01 00:11:43|2019-10-01 00:37:20|         264|         264|   NULL|                B00014|\n",
      "|              B00014|2019-10-01 00:56:29|2019-10-01 00:57:47|         264|         264|   NULL|                B00014|\n",
      "|              B00014|2019-10-01 00:23:09|2019-10-01 00:28:27|         264|         264|   NULL|                B00014|\n",
      "|     B00021         |2019-10-01 00:00:48|2019-10-01 00:07:12|         129|         129|   NULL|       B00021         |\n",
      "|     B00021         |2019-10-01 00:47:23|2019-10-01 00:53:25|          57|          57|   NULL|       B00021         |\n",
      "|     B00021         |2019-10-01 00:10:06|2019-10-01 00:19:50|         173|         173|   NULL|       B00021         |\n",
      "|     B00021         |2019-10-01 00:51:37|2019-10-01 01:06:14|         226|         226|   NULL|       B00021         |\n",
      "|     B00021         |2019-10-01 00:28:23|2019-10-01 00:34:33|          56|          56|   NULL|       B00021         |\n",
      "|     B00021         |2019-10-01 00:31:17|2019-10-01 00:51:52|          82|          82|   NULL|       B00021         |\n",
      "|              B00037|2019-10-01 00:07:41|2019-10-01 00:15:23|         264|          71|   NULL|                B00037|\n",
      "|              B00037|2019-10-01 00:13:38|2019-10-01 00:25:51|         264|          39|   NULL|                B00037|\n",
      "|              B00037|2019-10-01 00:42:40|2019-10-01 00:53:47|         264|         188|   NULL|                B00037|\n",
      "|              B00037|2019-10-01 00:58:46|2019-10-01 01:10:11|         264|          91|   NULL|                B00037|\n",
      "|              B00037|2019-10-01 00:09:49|2019-10-01 00:14:37|         264|          71|   NULL|                B00037|\n",
      "|              B00037|2019-10-01 00:22:35|2019-10-01 00:36:53|         264|          35|   NULL|                B00037|\n",
      "|              B00037|2019-10-01 00:54:27|2019-10-01 01:03:37|         264|          61|   NULL|                B00037|\n",
      "|              B00037|2019-10-01 00:08:12|2019-10-01 00:28:47|         264|         198|   NULL|                B00037|\n",
      "|              B00053|2019-10-01 00:05:24|2019-10-01 00:53:03|         264|         264|   NULL|                  #N/A|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fhv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1577500b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- dropOff_datetime: string (nullable = true)\n",
      " |-- PUlocationID: string (nullable = true)\n",
      " |-- DOlocationID: string (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fhv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d652cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv_pd = pd.read_csv('fhv_tripdata_2019-10.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42ccaeed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dispatching_base_num</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropOff_datetime</th>\n",
       "      <th>PUlocationID</th>\n",
       "      <th>DOlocationID</th>\n",
       "      <th>SR_Flag</th>\n",
       "      <th>Affiliated_base_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00009</td>\n",
       "      <td>2019-10-01 00:23:00</td>\n",
       "      <td>2019-10-01 00:35:00</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00013</td>\n",
       "      <td>2019-10-01 00:11:29</td>\n",
       "      <td>2019-10-01 00:13:22</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00014</td>\n",
       "      <td>2019-10-01 00:11:43</td>\n",
       "      <td>2019-10-01 00:37:20</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00014</td>\n",
       "      <td>2019-10-01 00:56:29</td>\n",
       "      <td>2019-10-01 00:57:47</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00014</td>\n",
       "      <td>2019-10-01 00:23:09</td>\n",
       "      <td>2019-10-01 00:28:27</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1897488</th>\n",
       "      <td>B03160</td>\n",
       "      <td>2019-10-31 23:38:00</td>\n",
       "      <td>2019-10-31 23:48:00</td>\n",
       "      <td>242.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B02887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1897489</th>\n",
       "      <td>B03160</td>\n",
       "      <td>2019-10-31 23:11:00</td>\n",
       "      <td>2019-10-31 23:43:00</td>\n",
       "      <td>161.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B02883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1897490</th>\n",
       "      <td>B03160</td>\n",
       "      <td>2019-10-31 23:13:00</td>\n",
       "      <td>2019-10-31 23:41:00</td>\n",
       "      <td>168.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B02883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1897491</th>\n",
       "      <td>B03186</td>\n",
       "      <td>2019-10-31 23:02:32</td>\n",
       "      <td>2019-10-31 23:09:53</td>\n",
       "      <td>264.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B03186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1897492</th>\n",
       "      <td>B03186</td>\n",
       "      <td>2019-10-31 23:19:24</td>\n",
       "      <td>2019-10-31 23:24:31</td>\n",
       "      <td>264.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B03186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1897493 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        dispatching_base_num      pickup_datetime     dropOff_datetime  \\\n",
       "0                     B00009  2019-10-01 00:23:00  2019-10-01 00:35:00   \n",
       "1                     B00013  2019-10-01 00:11:29  2019-10-01 00:13:22   \n",
       "2                     B00014  2019-10-01 00:11:43  2019-10-01 00:37:20   \n",
       "3                     B00014  2019-10-01 00:56:29  2019-10-01 00:57:47   \n",
       "4                     B00014  2019-10-01 00:23:09  2019-10-01 00:28:27   \n",
       "...                      ...                  ...                  ...   \n",
       "1897488               B03160  2019-10-31 23:38:00  2019-10-31 23:48:00   \n",
       "1897489               B03160  2019-10-31 23:11:00  2019-10-31 23:43:00   \n",
       "1897490               B03160  2019-10-31 23:13:00  2019-10-31 23:41:00   \n",
       "1897491               B03186  2019-10-31 23:02:32  2019-10-31 23:09:53   \n",
       "1897492               B03186  2019-10-31 23:19:24  2019-10-31 23:24:31   \n",
       "\n",
       "         PUlocationID  DOlocationID  SR_Flag Affiliated_base_number  \n",
       "0               264.0         264.0      NaN                 B00009  \n",
       "1               264.0         264.0      NaN                 B00013  \n",
       "2               264.0         264.0      NaN                 B00014  \n",
       "3               264.0         264.0      NaN                 B00014  \n",
       "4               264.0         264.0      NaN                 B00014  \n",
       "...               ...           ...      ...                    ...  \n",
       "1897488         242.0          81.0      NaN                 B02887  \n",
       "1897489         161.0          28.0      NaN                 B02883  \n",
       "1897490         168.0         215.0      NaN                 B02883  \n",
       "1897491         264.0         119.0      NaN                 B03186  \n",
       "1897492         264.0         119.0      NaN                 B03186  \n",
       "\n",
       "[1897493 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fhv_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70cbda56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dispatching_base_num       object\n",
       "pickup_datetime            object\n",
       "dropOff_datetime           object\n",
       "PUlocationID              float64\n",
       "DOlocationID              float64\n",
       "SR_Flag                   float64\n",
       "Affiliated_base_number     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fhv_pd.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca19c2fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[dispatching_base_num: string, pickup_datetime: string, dropOff_datetime: string, PUlocationID: double, DOlocationID: double, SR_Flag: double, Affiliated_base_number: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(df_fhv_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a3da55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv_pd.iteritems = df_fhv_pd.items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abe50dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropOff_datetime', StringType(), True), StructField('PUlocationID', DoubleType(), True), StructField('DOlocationID', DoubleType(), True), StructField('SR_Flag', DoubleType(), True), StructField('Affiliated_base_number', StringType(), True)])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(df_fhv_pd).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d067ad1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfeedf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fhv_schema = types.StructType([\n",
    "types.StructField('dispatching_base_num', types.StringType(), True), \n",
    "types.StructField('pickup_datetime', types.TimestampType(), True), \n",
    "types.StructField('dropOff_datetime', types.TimestampType(), True), \n",
    "types.StructField('PUlocationID', types.IntegerType(), True), \n",
    "types.StructField('DOlocationID', types.IntegerType(), True),\n",
    "types.StructField('SR_Flag', types.DoubleType(), True), \n",
    "types.StructField('Affiliated_base_number', types.StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1efb5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(fhv_schema) \\\n",
    "    .csv('fhv_tripdata_2019-10.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7a84e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropOff_datetime: timestamp (nullable = true)\n",
      " |-- PUlocationID: integer (nullable = true)\n",
      " |-- DOlocationID: integer (nullable = true)\n",
      " |-- SR_Flag: double (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fhv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "686b809b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_ALREADY_EXISTS] Path file:/Users/barbaraatkins/dev/projects/data-engineering-zoomcamp/05-batch/code/data/pq/fhv/2019/10 already exists. Set mode as \"overwrite\" to overwrite the existing path.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m df_fhv \\\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;241m.\u001b[39mrepartition(\u001b[38;5;241m6\u001b[39m) \\\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39mwrite \\\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/pq/fhv/2019/10\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/dev/projects/spark/spark-3.5.1-bin-hadoop3/python/pyspark/sql/readwriter.py:1721\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1719\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1720\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1721\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mparquet(path)\n",
      "File \u001b[0;32m~/dev/projects/spark/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/dev/projects/spark/spark-3.5.1-bin-hadoop3/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_ALREADY_EXISTS] Path file:/Users/barbaraatkins/dev/projects/data-engineering-zoomcamp/05-batch/code/data/pq/fhv/2019/10 already exists. Set mode as \"overwrite\" to overwrite the existing path."
     ]
    }
   ],
   "source": [
    "df_fhv \\\n",
    "    .repartition(6) \\\n",
    "    .write \\\n",
    "    .parquet('data/pq/fhv/2019/10') \\\n",
    "    .mode('overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43a1bf55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_fhv = spark.read.parquet('data/pq/fhv/2019/10/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18c9843c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropOff_datetime: timestamp (nullable = true)\n",
      " |-- PUlocationID: integer (nullable = true)\n",
      " |-- DOlocationID: integer (nullable = true)\n",
      " |-- SR_Flag: double (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fhv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02196978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dispatching_base_num',\n",
       " 'pickup_datetime',\n",
       " 'dropOff_datetime',\n",
       " 'PUlocationID',\n",
       " 'DOlocationID',\n",
       " 'SR_Flag',\n",
       " 'Affiliated_base_number']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fhv.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8ae344b",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df_fhv.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f222f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B02594|2019-10-19 12:32:00|2019-10-19 13:03:00|         157|         133|   NULL|                B02594|\n",
      "|              B00647|2019-10-07 19:26:49|2019-10-07 19:42:43|         264|          78|   NULL|                B00647|\n",
      "|              B01145|2019-10-20 02:30:04|2019-10-20 02:37:54|         264|         244|   NULL|                B01145|\n",
      "|              B03060|2019-10-13 18:58:21|2019-10-13 19:11:24|         264|         123|   NULL|                B02888|\n",
      "|              B02418|2019-10-07 21:40:00|2019-10-07 22:22:00|         264|         264|   NULL|                B00280|\n",
      "|              B02243|2019-10-18 11:08:00|2019-10-18 14:18:00|         156|         144|   NULL|                LX-188|\n",
      "|              B02826|2019-10-16 13:20:28|2019-10-16 13:25:06|         264|          97|   NULL|                B02826|\n",
      "|              B00856|2019-10-12 22:25:05|2019-10-12 22:48:59|         264|          91|   NULL|                B02875|\n",
      "|     B02462         |2019-10-08 20:24:59|2019-10-08 20:33:54|          92|          92|   NULL|       B02462         |\n",
      "|              B00221|2019-10-25 09:33:01|2019-10-25 09:45:16|         264|         119|   NULL|                B00221|\n",
      "|              B01145|2019-10-25 14:53:10|2019-10-25 15:11:20|         264|          69|   NULL|                B01145|\n",
      "|              B01877|2019-10-08 00:51:51|2019-10-08 00:51:56|         264|         264|   NULL|                B00306|\n",
      "|              B03016|2019-10-22 06:09:38|2019-10-22 06:16:19|         264|         174|   NULL|                B02932|\n",
      "|              B02546|2019-10-28 17:48:24|2019-10-28 17:55:10|         264|         248|   NULL|                B02546|\n",
      "|              B01239|2019-10-20 20:50:19|2019-10-20 21:09:26|         264|          69|   NULL|                B02682|\n",
      "|              B02429|2019-10-08 15:18:16|2019-10-08 16:14:26|         264|         264|   NULL|                B02875|\n",
      "|              B01340|2019-10-31 23:22:38|2019-10-31 23:25:52|         264|         213|   NULL|                B01340|\n",
      "|              B01318|2019-10-28 08:56:46|2019-10-28 09:15:15|         264|         164|   NULL|                B01318|\n",
      "|              B00856|2019-10-12 11:16:53|2019-10-12 11:22:15|         264|         155|   NULL|                B00856|\n",
      "|              B02157|2019-10-13 09:01:37|2019-10-13 10:06:37|         264|         265|   NULL|                B02157|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fhv.select(columns).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3c4afb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv.registerTempTable('fhv_trips_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a8cec419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   62610|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "          SELECT count(*) FROM fhv_trips_data WHERE DATE(pickup_datetime) = '2019-10-15'\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e1d06411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-----------------------------------------------------------------------------------------------------------------------+\n",
      "|    pickup_datetime|   dropoff_datetime|((unix_timestamp(dropoff_datetime, yyyy-MM-dd HH:mm:ss) - unix_timestamp(pickup_datetime, yyyy-MM-dd HH:mm:ss)) / 3600)|\n",
      "+-------------------+-------------------+-----------------------------------------------------------------------------------------------------------------------+\n",
      "|2019-10-11 18:00:00|2091-10-11 18:30:00|                                                                                                               631152.5|\n",
      "|2019-10-28 09:00:00|2091-10-28 09:30:00|                                                                                                               631152.5|\n",
      "|2019-10-31 23:46:33|2029-11-01 00:13:00|                                                                                                      87672.44083333333|\n",
      "|2019-10-01 21:43:42|2027-10-01 21:45:23|                                                                                                      70128.02805555555|\n",
      "|2019-10-17 14:00:00|2020-10-18 00:00:00|                                                                                                                 8794.0|\n",
      "|2019-10-26 21:26:00|2020-10-26 21:36:00|                                                                                                      8784.166666666666|\n",
      "|2019-10-30 12:30:04|2019-12-30 13:02:08|                                                                                                     1465.5344444444445|\n",
      "|2019-10-25 07:04:57|2019-12-08 07:54:33|                                                                                                     1057.8266666666666|\n",
      "|2019-10-25 07:04:57|2019-12-08 07:21:11|                                                                                                     1057.2705555555556|\n",
      "|2019-10-01 13:47:17|2019-11-03 15:20:28|                                                                                                      794.5530555555556|\n",
      "|2019-10-01 07:21:12|2019-11-03 08:44:21|                                                                                                      794.3858333333334|\n",
      "|2019-10-01 13:41:00|2019-11-03 14:58:51|                                                                                                               794.2975|\n",
      "|2019-10-01 18:43:20|2019-11-03 19:43:13|                                                                                                      793.9980555555555|\n",
      "|2019-10-01 18:43:46|2019-11-03 19:43:04|                                                                                                      793.9883333333333|\n",
      "|2019-10-01 07:07:09|2019-11-03 07:58:46|                                                                                                      793.8602777777778|\n",
      "|2019-10-01 14:49:28|2019-11-03 15:38:07|                                                                                                      793.8108333333333|\n",
      "|2019-10-01 05:36:30|2019-11-03 06:23:36|                                                                                                                793.785|\n",
      "|2019-10-01 15:02:55|2019-11-03 15:49:05|                                                                                                      793.7694444444444|\n",
      "|2019-10-01 06:08:01|2019-11-03 06:53:15|                                                                                                      793.7538888888889|\n",
      "|2019-10-01 06:41:17|2019-11-03 07:26:04|                                                                                                      793.7463888888889|\n",
      "+-------------------+-------------------+-----------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "          SELECT pickup_datetime, dropoff_datetime, (unix_timestamp(dropoff_datetime)-unix_timestamp(pickup_datetime))/(3600) FROM fhv_trips_data\n",
    "          ORDER BY 3 desc\n",
    "          \n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d63dd3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('taxi_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f02e8c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zones.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ff11b607",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones.registerTempTable('zone_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5d197c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     265|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "          SELECT count(*) FROM zone_data\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "23ce5141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|count(1)|                Zone|\n",
      "+--------+--------------------+\n",
      "|       1|         Jamaica Bay|\n",
      "|       2|Governor's Island...|\n",
      "|       5| Green-Wood Cemetery|\n",
      "|       8|       Broad Channel|\n",
      "|      14|     Highbridge Park|\n",
      "|      15|        Battery Park|\n",
      "|      23|Saint Michaels Ce...|\n",
      "|      25|Breezy Point/Fort...|\n",
      "|      26|Marine Park/Floyd...|\n",
      "|      29|        Astoria Park|\n",
      "|      39|    Inwood Hill Park|\n",
      "|      47|       Willets Point|\n",
      "|      53|Forest Park/Highl...|\n",
      "|      57|  Brooklyn Navy Yard|\n",
      "|      62|        Crotona Park|\n",
      "|      77|        Country Club|\n",
      "|      89|     Freshkills Park|\n",
      "|      98|       Prospect Park|\n",
      "|     105|     Columbia Street|\n",
      "|     110|  South Williamsburg|\n",
      "+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "          SELECT count(*), Zone from fhv_trips_data fhv, zone_data zd \n",
    "          WHERE fhv.PUlocationID = zd.LocationID\n",
    "          GROUP BY Zone\n",
    "          ORDER BY 1 \n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ea5a76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
